---
title: "Modelos de clasificación parte 1."
description: "Veremos dos modelos clásicos de
clasificación: árbol de decisión y vecinos más cercanos."
pubDate: "May 04 2023"
heroImage: "/placeholder-hero.jpg"
slug: 'classification_models/1'
---

---

Para abordar estos modelos, estaremos trabajando con un dataset clásico:
`Iris`.
[Aquí](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html?highlight=iris)
podrán encontrar más información respecto a cómo se compone este
conjunto de datos.

Scikit-Learn ya trae algunos datasets integrados. Para ello, debemos
importarlos desde el módulo *sklearn.datasets*.

``` python
# Importamos las librerías que estaremos utilizando

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```

``` python
# Cargamos el dataset de Iris

from sklearn.datasets import load_iris
iris = load_iris()
```

Si imprimen la variable \"iris\", podrán observar que no tiene una
estructura de DataFrame. Para ello, debemos convertirla con la libería
Pandas.

``` python
data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                     columns= iris['feature_names'] + ['target'])
data.head()
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

*Ahora tenemos nuestro dataset Iris expresado en una estructura
DataFrame de Pandas.*

### `Pairplot`

En esta instancia, emplearemos un paso fundamental dentro de todo
**EDA** que nos muestra todas las relaciones entre las variables
cuantitativas del dataframe que estemos trabajando. La importancia de
este paso radica en que nos permite elegir las mejores variables
predictoras para nuestro modelo. Si queremos predecir la variedad de una
flor, lo más racional sería seleccionar aquellos atributos que tengan
una mayor incidencia en que una flor pertenezca a una u otra categoría.

Como se desprende de lo comentado, este paso es condición *sine qua non*
en cualquier **Análisis Exploratorio de Datos** que emprendan.

``` python
sns.pairplot(data, hue="target", palette = ['r', 'g', 'b'])
plt.show()
```

<img src="/m2/1/65fab16c6be32602bf1ef3abc59a5824cea02042.png" alt="Modelos de clasificaciòn" />

Separamos del dataframe dos atributos y las etiquetas. Llamaremos
**`X`** a los features e **`y`** a las etiquetas.

Eligiremos dos atributos: *petal length* y *petal width*. Estos parecen
tener un gran poder predictor para separar las clases correctamente.
Luego de correr todo el notebook, y prestar atención al funcionamiento
del modelo de Machine Learning, recomendamos que repitan el proceso con
otros dos atributos de su elección para que corroboren la influencia que
tiene la selección de variables predictoras en el funcionamiento de un
modelo.

``` python
X = data[['petal length (cm)', 'petal width (cm)']]  
# Denotamos X con mayúscula ya que 
# incluye más de un atributo
y = data.target # Etiqueta a predecir
y
```

    0      0.0
    1      0.0
    2      0.0
    3      0.0
    4      0.0
          ... 
    145    2.0
    146    2.0
    147    2.0
    148    2.0
    149    2.0
    Name: target, Length: 150, dtype: float64

``` python
X.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.3</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.5</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.4</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>

``` python
y.tail()
```

    145    2.0
    146    2.0
    147    2.0
    148    2.0
    149    2.0
    Name: target, dtype: float64

## **1. Árbol de decisión**

Ahora que tenemos nuestras variables predictoras `X`, por un lado, y
nuestra varible a predecir `y`, por el otro, vamos a crear un modelo de
árbol de decisión.

Recordemos el flujo de trabajo de ML en Scikit-Learn: creamos el modelo
/ entrenamos / predecimos.

``` python
from sklearn.tree import DecisionTreeClassifier

# Instanciamos un objeto de la clase DecisionTreeClassifier

clf = DecisionTreeClassifier(max_depth = 3, random_state = 42) 
```

Notar que establecimos max_depth en 3.

Esto es un `hiperparámtro` que define la profundidad que tendrá nuestro
árbol, es decir, cuántos niveles de preguntas hará hasta que llegue a
una hoja donde asigne una etiqueta.

A continuación, esquematizamos la profundidad de un árbol:

<img src="\assets\profundidad_arbol.png">

A, B, C, etc. representan las preguntas que va haciendo el árbol en cada
subdivisión por sus ramas. En efecto, cada vez que baja un nivel,
aumenta su profundidad.

**Para facilitar la comprensión de este notebook, omitiremos la
separación de nuestro dataset en set de entrenamiento/set de testeo.**

``` python
# Entrenamos el modelo

clf.fit(X.values,y.values)
```

``` python
# Exploramos algunas de las características del modelo entrenado. 
# Para pensar, 
# ¿qúe representa cada uno de estos valores?

print(clf.classes_)
print(clf.n_classes_)
print(clf.max_features_)
print(clf.feature_importances_)
```

    [0. 1. 2.]
    3
    2
    [0.58561555 0.41438445]

``` python
# Predecimos

y_pred = clf.predict(X.values)
```

    ---------------------------------------------------------------------------
    NameError                                 Traceback (most recent call last)
    Cell In[11], line 3
          1 # Predecimos
    ----> 3 y_pred = clf.predict(X.values)

    NameError: name 'clf' is not defined

Ahora **evaluaremos** nuestro modelo de clasificación. No te preocupes,
la próxima clase veremos más en profundiad cada una de las métricas
existentes para los problemas de clasificación. Un adelanto: existe algo
llamado matriz de confusión, que tiene una serie de métricas muy
importantes, entre ellas la precisión (*acurracy*).

``` python
from sklearn.metrics import accuracy_score

print(accuracy_score(y,y_pred))
```

    0.96

Observemos, en esta etapa, las fronteras de clasificación que estableció
nuestro modelo.

``` python
plt.figure(figsize= (10,6))


ax = sns.scatterplot(x = X.iloc[:,0], y = X.iloc[:,1], hue= y, palette='Set2')
plt.legend().remove()


xlim = ax.get_xlim()
ylim = ax.get_ylim()
xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
                      np.linspace(*ylim, num=200))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

contours = ax.contourf(xx, yy, Z, alpha=0.3, cmap = 'Set2')
plt.tight_layout()
plt.title('Fronteras de decisión', fontsize= 15)
plt.show()
```

<img src="/m2/1/faa99386db7e8abcbc2cf1691f3751b30b2525f6.png" alt="Modelos de clasificaciòn" />

*Probá modificar la profundiad del árbol para analizar su impacto en el
establecimiento de las fronteras de decisión por parte de nuestor
clasificador.*

A priori, si aumentaras su profundidad, ¿pensás que será más o menos
preciso? ¿Se ajustará más o menos a los datos?

### Visualizamos el árbol de decisión

``` python
from sklearn import tree
```

``` python
plt.figure(figsize = (10,7))
tree.plot_tree(clf, filled = True)
plt.show()
```

<img src="/m2/1/6b11387d9a8af880f69ae6e176366f587102cc77.png" alt="Modelos de clasificaciòn" />

``` python
# Ploteamos la importancia de los atributos elegidos a la hora de hacer la predicción


plt.figure(figsize = (8,5))

importances = clf.feature_importances_
columns = X.columns
sns.barplot(x = columns, y = importances)
plt.title('Importancia de cada feature', fontsize = 15)
plt.show()
```

<img src="/m2/1/144fd44b5420b791ff8eca0ee0c34233db586b11.png" alt="Modelos de clasificaciòn" />

## **2. Vecinos más cercanos** {#2-vecinos-más-cercanos}

``` python
from sklearn.neighbors import KNeighborsClassifier

# Instanciamos un objeto de la clase KNeighborsClassifier

clf = KNeighborsClassifier(n_neighbors=8)
```

``` python
# Entrenamos

clf.fit(X.values,y.values)
```

``` python
# Predecimos

y_pred = clf.predict(X.values)
```

``` python
# Evaluamos

print(accuracy_score(y,y_pred))
```

    0.9666666666666667

Observermos, en esta etapa, las fronteras de clasificación que fijó
nuestro modelo.

``` python
plt.figure(figsize= (10,6))
ax = sns.scatterplot(x = X.iloc[:,0], y = X.iloc[:,1], hue=y, palette='
                     
                     ')
plt.legend().remove()


xlim = ax.get_xlim()
ylim = ax.get_ylim()
xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
                      np.linspace(*ylim, num=200))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

contours = ax.contourf(xx, yy, Z, alpha=0.3, cmap = 'Set2')
plt.tight_layout()
# plt.savefig('arbol_iris.png', dpi = 400)
plt.show()
```

<img src="/m2/1/4d598ff26f6a4850731beb5ea2a43cd80592166b.png" alt="Modelos de clasificaciòn" />

*Probá modificar la cantidad de vecinos **k** para analizar su impacto
en el establecimiento de las fronteras de decisión por parte de nuestro
clasificador.*

Para ello, averiguar sobre el hiperparámetro `n_neighbors` de
KNeighborsClassifier.
[Aquí](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
les dejamos la documentación.
