---
title: 'Las expresiones regulares y la segmentaci贸n de palabras y frases'
description: ''
pubDate: 'Nov 08 2022'
heroImage: '/placeholder-hero.jpg'
url: '/ia'
---

Los primeros agentes conversacionales estaban basados en regex, ejecutados a lo largo de un corpus y observando los resultados

## 驴Qu茅 es una palabra?

Una palabra es todo conjunto de caracteres rodeado por espacios o signos de puntaci贸n, pero cuando trabajamos con transcipciones de conversaciones humanas tenemos:

"Ehhh bueno, yo vivo en en sidney y esto... no no me llega a convencer la cantidad de gente que hay"

A la hora de analizar palabras tenemos:

- _Tipos:_ El n煤mero de tipos de palabras es la cantidad de palabras diferentes que encontramos en un texto.
- _Tokens:_ El n煤mero total de palabras que encontramos en un texto.

La tokenizaci贸n consiste en encontrar todas las palabras (que cumplan con nuetra definici贸n de palabra como dev del sistema) y el an谩lisis de tipos de encontrar el conjunto de palabras 煤nicas que se emplean en un texto.

### Normalizaci贸n del texto

Facilita el proceso posterior de nuestros algoritmos encontrando componentes de nuestro texto y aplicandoles una serie de procesos que nos permitan obtener una salida homog茅nea.

1. Tokenizaci贸n
2. Normalizaci贸n del formato de las palabras
3. Segmentaci贸n de frases y oraciones

### Tokenizaci贸n

Es el proceso de encontrar los elementos b谩sicos individiales de un texto, habitualmente las palabras.

Por ejemplo para la frase "Ninguna ciencia, en cianto a ciencia,enga帽谩;el enga帽o est谩 en quien no la sabe" si usamos el criterio de separar por espacios y puntuaci贸nes, tenemos 15 tokens en total cada uno en una fila

Esta l贸gica formal que persigue tokenizar palabras en cualquier idioma tiene por ejemplo este problema de los ap贸strofes:

```bash
you're young -> you are young
J'aime le gateau -> Je aime le gateau
```

Uno de los algoritmos de tokenizaci贸n es Penn Treebank que usa regex por su precisi贸n y rapidez siendo simb贸licos y no estad铆sticos por motivos de eficiencia, pero tambi茅n est谩n los basados en ML para tokenizar texto.

El espa帽ol requiere mayor esfuerzo que el ingl茅s o frances.

Los modelos puede que no sepan todas las palabras de un idioma, por lo que debemos encontrar una forma de detectar palabras que nuestros modelos no hayan procesado previamente y tokenizarlas aqu铆 entra el concepto de _subword_ que permite detectar palabras que no han sido nunca procesadas.

Un algoritmo conocido para este fin es el _byte-pair encoding (BPE)_ de 2016, segmenta cada caracter y analiza aquellos que aparecen uno junto al otro la mayor parte de las veces. Una vez detectadas estas estructuras, sustituye estos dos tokens por el token construido por el conjunto de dos caracteres. Repitiendose hasta alcanzar a el parametro _k_.

```bash
# corpus:
escribir, estructura, partir, partici贸n, dormir
# BPE toma todos los caracteres como vocavulario
e,s,c,r,i,b,t,u,a,p,o,d,m
# luego busca los que mas se repiten en el corpus y lo a帽ade al vocavulario
e,s,c,r,i,b,t,u,a,p,o,d,m,ir_
# el segundo conjunto es 'es':
e,s,c,r,i,b,t,u,a,p,o,d,m,ir_, \_es
# y el corpus seria(representa salto de linea 路 ):
_es路c路r路i路b路ir_路,路_es路t路r路u路c路t路u路r路a路,路p路a路r路t路ir_,路p路a路r路t路i路c路i路贸路n路,路d路o路r路m路ir_
# iterando k, encontrara "_part"
e,s,c,r,i,b,t,u,a,p,o,d,m,ir_, \_es, _part

# nuestro corpus quedaria
_es路c路r路i路b路ir_路,路_es路t路r路u路c路t路u路r路a路,_part路ir_,路_part路i路c路i路贸路n路,路d路o路r路m路ir_
```

---

### Normalizaci贸n del Formato de las Palabras en NLP

En el procesamiento de lenguaje natural (NLP), una misma palabra puede representarse de diferentes formas: `"monitor"`, `"Monitor"`, `"MONITOR"`. Un tokenizador ingenuo tratar铆a cada una como un token distinto, lo que afecta la eficiencia del modelo. Para evitar esto, se aplica el **case folding**, que convierte todas las palabras a min煤sculas.

Sin embargo, esta t茅cnica no es suficiente en todos los casos. Por ejemplo, en un corpus podemos encontrar: `"democracia"`, `"democr谩tico"`, `"dem贸crata"`. Dependiendo del caso de uso, puede ser conveniente unificar todos estos t茅rminos bajo un 煤nico token, como `"democracia"`. Aqu铆 es donde entra en juego el **stemming**.

**Stemming: Ra铆ces de las Palabras**

El stemming es un m茅todo basado en heur铆sticas que elimina las terminaciones derivativas m谩s comunes para obtener la ra铆z de una palabra. Algunas de sus caracter铆sticas clave son:

- Se basa en reglas predefinidas del idioma.
- No garantiza ra铆ces ling眉铆sticamente correctas, sino recortes funcionales.
- Su precisi贸n depende del dise帽o del algoritmo y del idioma tratado.

El **stemming** funciona bien cuando los tokens comparten una misma ra铆z visible, pero tiene limitaciones. Por ejemplo, un stemmer no identificar铆a que `"fui"`, `"ir谩n"` y `"vamos"` derivan del verbo `"ir"`. Para estos casos se utiliza una t茅cnica m谩s avanzada: la **lematizaci贸n**.

**Lematizaci贸n: An谩lisis Morfol贸gico Avanzado**

A diferencia del stemming, la **lematizaci贸n** no solo recorta palabras, sino que analiza su estructura morfol贸gica y significado gramatical. Esto permite identificar correctamente las formas derivadas de una palabra.

Por ejemplo, en la palabra `"submarino"` se pueden identificar tres morfemas:

- **"sub-"** (prefijo: debajo de).
- **"mar"** (ra铆z: hace referencia al mar).
- **"-ino"** (sufijo: perteneciente a).

Para lograr esto, los algoritmos de lematizaci贸n utilizan bases de datos ling眉铆sticas y modelos estad铆sticos, analizando grandes vol煤menes de texto para identificar patrones y estructuras comunes.

El stemming es m谩s r谩pido pero menos preciso, la lematizaci贸n ofrece una mayor exactitud a costa de mayor complejidad computacional.

La elecci贸n entre ambos m茅todos depender谩 de las necesidades del proyecto y de los recursos disponibles. 

---

### Segmentaci贸n de Frases y Oraciones en NLP

Hasta ahora, la **tokenizaci贸n** nos ha permitido extraer las palabras individuales de un texto y construir un vocabulario. Adem谩s, con t茅cnicas como el **stemming** y la **lematizaci贸n**, hemos logrado unificar palabras derivadas de una misma ra铆z para mejorar la comprensi贸n del sistema, siempre que esto sea adecuado seg煤n el caso de uso.

Si las palabras son la unidad m铆nima del lenguaje, las **oraciones** representan estructuras completas con significado. Segmentarlas correctamente es fundamental para el an谩lisis de textos en NLP.

_De Reglas a Aprendizaje Autom谩tico_

Los primeros enfoques para separar oraciones se basaban en reglas predefinidas, pero enfrentaban problemas con ambig眉edades ling眉铆sticas. A principios del siglo XXI, comenzaron a desarrollarse modelos basados en **aprendizaje autom谩tico (ML)** que automatizan este proceso.

Uno de los m谩s destacados fue **Punkt**, publicado en 2006, que utilizaba un enfoque de **aprendizaje no supervisado**. Su gran ventaja es que puede reentrenarse con cualquier corpus, adapt谩ndose a distintos dominios y estilos de escritura.

_Construyendo un Texto Comprensible para NLP_

Pasamos de un conjunto de caracteres a un vocabulario representado por **tokens**, que luego normalizamos para evitar variaciones innecesarias. Finalmente, agrupamos estos tokens en **oraciones bien segmentadas**, listas para su procesamiento.

_Importancia de una Buena Tokenizaci贸n y Segmentaci贸n_

Muchos algoritmos avanzados, como los de **an谩lisis de sentimientos**, dependen de una tokenizaci贸n y normalizaci贸n precisas. Si estos procesos no son 贸ptimos, los resultados ser谩n inexactos, haciendo que el modelo falle o sea descartado. A menudo, el problema no es el modelo en s铆, sino una segmentaci贸n deficiente, una mala elecci贸n de tokenizador o la falta de una adecuada lematizaci贸n.

Conocer estos fundamentos es clave para mejorar el rendimiento de cualquier sistema de NLP. 
