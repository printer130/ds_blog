---
title: 'NLP 3: Categorizaci√≥n, autocorreci√≥n y autocompletado de texto'
description: ''
pubDate: 'Nov 08 2022'
heroImage: '/placeholder-hero.jpg'
url: '/ia'
---

## Autocorrecci√≥n y Algoritmos de Estimaci√≥n de Distancia

La **autocorrecci√≥n** es una aplicaci√≥n avanzada de NLP que corrige palabras mal escritas y sugiere la versi√≥n correcta. Presente en la mayor√≠a de los sistemas de mensajer√≠a, su funcionamiento se basa en cuatro pasos principales:

1. Identificaci√≥n de palabras fuera del vocabulario.
2. B√∫squeda de palabras similares dentro del vocabulario conocido.
3. Sustituci√≥n de la palabra err√≥nea por la m√°s adecuada.
4. Evaluaci√≥n del contexto para determinar la mejor correcci√≥n posible.

## 1 - Estimaci√≥n de la Distancia entre Palabras

La **distancia de edici√≥n** mide cu√°ntas modificaciones se requieren para transformar una palabra en otra. Se basa en cuatro operaciones fundamentales:

- **Inserci√≥n (Insert)**: A√±adir una letra.
- **Eliminaci√≥n (Delete)**: Quitar una letra.
- **Intercambio (Switch)**: Reordenar letras contiguas.
- **Sustituci√≥n (Replace)**: Reemplazar una letra por otra.

El algoritmo **Minimum Edit Distance (MED)** calcula la menor cantidad de cambios necesarios asignando un costo a cada operaci√≥n.

### **Ejemplo**

Transformar _"casa"_ en _"osa"_ requiere:

1. Eliminaci√≥n de _"c"_ ‚Üí _"asa"_.
2. Sustituci√≥n de _"a"_ por _"o"_ ‚Üí _"osa"_.

El costo total es **2**, indicando una distancia de edici√≥n de **2**.

## 2 - Filtrado de Candidatos y Estimaci√≥n de Probabilidad

Para optimizar el procesamiento, se utiliza un enfoque probabil√≠stico en lugar de un modelo basado en segmentaci√≥n.

La probabilidad de que una palabra aparezca en un corpus se estima como:

\[
P(w) = \frac{C(w)}{V}
\]

Donde:

- **P(w)** es la probabilidad de la palabra en el corpus.
- **C(w)** es la cantidad de veces que la palabra aparece.
- **V** es el total de palabras en el corpus.

La palabra con mayor probabilidad se selecciona como la mejor opci√≥n para la autocorrecci√≥n.

---

## Categorizaci√≥n de Palabras (Part of Speech Tagging)

El **Part of Speech Tagging (POS Tagging)** es el proceso de clasificar palabras seg√∫n su funci√≥n gramatical dentro de un texto. Este proceso asigna autom√°ticamente etiquetas a cada token, facilitando el an√°lisis ling√º√≠stico en tareas de procesamiento de lenguaje natural (NLP).

### **Clases l√©xicas en NLP**

En NLP, las categor√≠as l√©xicas se dividen en dos tipos:

#### **Open Classes (Clases Abiertas)**

Son aquellas que evolucionan constantemente con la lengua y admiten la incorporaci√≥n de nuevas palabras:

- **Sustantivos**: Designan entidades fijas o conceptos concretos.
- **Adjetivos**: Expresan cualidades o caracter√≠sticas de un sustantivo.
- **Verbos**: Representan acciones, procesos o estados.
- **Adverbios**: Modifican verbos, adjetivos u otros adverbios, indicando lugar, tiempo, modo o cantidad.

#### **Closed Classes (Clases Cerradas)**

Estas categor√≠as son m√°s estables y no suelen incorporar nuevos elementos:

- Pronombres
- Preposiciones
- Conjunciones
- Determinantes
- Interjecciones

### **Desambiguaci√≥n en POS Tagging**

Un desaf√≠o clave del POS tagging es la desambiguaci√≥n de palabras con m√∫ltiples funciones gramaticales. Algunos ejemplos:

- **"bajo"** puede ser una preposici√≥n ("bajo el cielo") o un adjetivo ("el piso de abajo").
- **"ante"** puede actuar como preposici√≥n ("ante √©l") o como sustantivo ("prendas de ante").

### **Modelos de POS Tagging y Evaluaci√≥n**

El objetivo es desarrollar modelos m√°s precisos que el enfoque basado en la **Most Frequent Class Baseline**, que simplemente asigna la categor√≠a m√°s com√∫n seg√∫n un corpus etiquetado. Este m√©todo tiene una precisi√≥n del **92%**, mientras que un modelo m√°s avanzado debe alcanzar al menos un **97%** de precisi√≥n para ser competitivo.

La mejora en POS tagging es clave para aplicaciones de NLP como la traducci√≥n autom√°tica, el an√°lisis de sentimientos y la extracci√≥n de informaci√≥n, donde una correcta clasificaci√≥n gramatical es fundamental.

---

### Hidden Markov chains y el algoritmo Viterbi

Trata de modelar situaciones en las que es necesario predecir la probabilidad de pasar de un estado a otro en un sistema dado (clima), con esto estamos tratando de implementar un algoritmo para clasificar parted del discurso, o un POS tagger, que nuestro algoritmo trabajara con palabras a las que tendra que asociar clases lexicas.

Los estados se representan como circulo:

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741379424/markov_vbmvak.jpg" alt="markov">

De nublado a lluvioso hay 50%, de nublado a nublado hay 30%, de nublado a soleado hay 20% que se cumpla el clima para ma√±ana.

En una cadena de Markov para POS tagging definiremos la probabilidad de transicion desde una categoria de palabra hasta otra, por ejemplo vemos si estamos en nombre la siguiente palabra sea un adjetivo con 60%:

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741379866/markov0_f1caue.jpg" alt="Markov">

Con esto aparecen lo Hidden Markov Models(HMM) donde el sistema desconoce las etiquetas de las palabras, entonces no puede saber el estado, por otro lado estan los estados observables los que conoce el sistema y son las palabras de nuestro corpus, y los no observables que son las categorias del part of speech (verbos,nombres,adjetivos,adverbios) dado que no se pueden conocer sencillamente observando la palabra.

Entonces tenemos dado un corpus de entrenamiento con el que deseemos generar nuestro algoritmo de POS tagging, tendremos que calcular la matriz de emision y la matriz de transicion de dicho corpus para generar entonces un Hidden Markov Model que represente nuestro corpus y sus categorias lexicas y palabras.

-

## Cadenas de Markov Ocultas y el Algoritmo Viterbi

Los **Hidden Markov Models (HMM)** son modelos probabil√≠sticos utilizados para predecir la transici√≥n entre diferentes estados en un sistema. Un ejemplo com√∫n es la predicci√≥n del clima:

- **Probabilidad de que ma√±ana sea lluvioso** si hoy est√° nublado: **50%**.
- **Probabilidad de que ma√±ana siga nublado**: **30%**.
- **Probabilidad de que ma√±ana sea soleado**: **20%**.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741379424/markov_vbmvak.jpg" alt="Modelo de Markov">

### Aplicaci√≥n en POS Tagging

En **etiquetado de partes del discurso (POS tagging)**, se utiliza una cadena de Markov para modelar la probabilidad de transici√≥n entre categor√≠as gramaticales.

Por ejemplo, si una palabra es un **sustantivo**, la siguiente palabra tiene un **60% de probabilidad** de ser un **adjetivo**.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741379866/markov0_f1caue.jpg alt="Transici√≥n de categor√≠as" />

### Hidden Markov Models (HMM)

En un HMM, el sistema trabaja con:

- **Estados ocultos**: Las categor√≠as gramaticales (**sustantivos, verbos, adjetivos, etc.**) que el modelo intenta inferir.
- **Estados observables**: Las palabras del corpus, cuya categor√≠a no es directamente conocida.

Para entrenar un **POS tagger basado en HMM**, es necesario calcular:

- **Matriz de transici√≥n:** Probabilidad de pasar de una categor√≠a a otra.
- **Matriz de emisi√≥n:** Probabilidad de que una palabra pertenezca a una categor√≠a espec√≠fica.

Con estos datos, el **algoritmo de Viterbi** permite encontrar la secuencia de etiquetas m√°s probable para una oraci√≥n, mejorando la precisi√≥n del etiquetado gramatical. üöÄ

-

Para el metodo de generacion de matrices tenemos este ejemplo:

Hoy[ADV] ha[VER] muerto[VER] mama[NOM] o[CONJ] quiza[ADV] ayer[ADV]. No[ADV] lo[PRON] se[VER]. Recibi[VER] un[DET] telegrama[NOM] del[PREP|ART] asilo[NOM]

Para la _matriz de transicion_ calculamos ADVERBIO √° VERBO, sera sumar la cantidad de veces que sucede la transicion de ADV √° VER y dividirla entre el total de transiciones partiendo de ADV hacia otro en nuestro corpus:

Encontramos 1 transicion ADV √° VER de entre 4 transiciones: P(ADV,VER) = 1/4 = .25

De ADV √° ADV tenemos 2 transiciones: P(ADV, ADV)= 2/4 = .5

De P(ADV, PRON) = 1/4 = .25

Haciendo esto para todas las posibles transiciones desde cuaquier categoria a cualquier categoria tendremos la tabla de probabilidades de transicion que indica la probabilidad de pasar de un estado no observable hasta otro estado no observable.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741381945/matriztransicion_mtynmd.jpg" alt="Matriz de transici√≥n">

Para la _matriz de emisi√≥n_ que es la que representa las probabilidades de pasar desde un estado oculto(categoria de palabra) hacia un estado visible (una palabra concreta), para las columnas tendremos las palabras de nuestro corpus y en las filas volveran a estar las categorias, entonces para calcular la probabilidad de emision desde el estado oculto NOM hasta la palabra contreta mama, vemos cuantas veces una etiqueta 'nombre' tiene como palabra asociada 'mama' y dividirlo entre el total de las palabras etiquetadas como nombre:

```math
C(NOM, 'mama') = 1/3 = .33
C(NOM, 'asilo') = 1/3 = .33
C(NOM, 'telegrama') = 1/3 = .33
```

Con este corpus peque√±o no se obtienen resultados con sentido, pero nos ayuda a entender como pasar de un corpus etiquetado a una matriz de transicion y a una matriz de emision, que son importantes para un sistema de Parts of Speech Taggin mediante Hidden Markov Model.

Para clasificar palabras basandonos en esto tenemos el _algoritmo de viterbi_ que trabaja con HMM y trata de encontrar la secuencia mas probable de estados ocultos, lo que llamamos camino de Viterbi. En nuestro ejemplo trata de estimar la secuencia mas probable de categorias de palabras dada una frase.

Funciona as√≠ en cada estado multiplica la probabilidad de pasar del estado actual a cualquiera de los estados ocultos que pueden llegar a emitir el estado visible. Luego seleccionar√° el estado oculto futuro que m√°s probabilidad alcance.

```math
P(estado_actual, estado_futuro) x C(estado_futuro, estado_visible)
```

En este caso es la probabilidad de que desde una categor√≠a de palabra pasemos a otra multiplicada por la probabilidad de que la siguiente palabra de la frase pertenezca a esta nueva categor√≠a.

Partiendo de nuestras 2 matrices podemos estimar las categor√≠as de las palabras de una frase. As√≠ conocemos el estado en el que estamos (la categor√≠a de la palabra anterior de la frase) y la palabra actual. Tendremos que encontrar el estado futuro (la categor√≠a de la palabra actual) en base a nuestras dos matrices de probabilidades que han sido extra√≠das de un corpus empleando un HMM.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741388363/markov1_kowl2f.webp" alt="Markov">

Empezamos por la primera palabra de "coto privado de caza"

La matriz de arriba es la **matriz de transici√≥n**, la probabilidad de moverse de un estado a otro.  
La matriz inferior es la **matriz de emisi√≥n**, indica la probabilidad de que un estado o etiqueta (como NOM, VERB, etc.) genere una palabra en la oraci√≥n.

**Entonces para c√°lcular "coto"**:

1. **Transici√≥n desde el estado inicial (Estado 0) a NOM (Sustantivo)**:

   - **P(Estado 0 ‚Üí NOM) = 0.5** (tomado de la primera fila de la matriz de transici√≥n)

2. **Emisi√≥n de "coto" desde NOM**:

   - **C(NOM, "coto") = 1.0** (porque en la matriz de emisi√≥n, la suma de probabilidades de cada columna debe ser 1, y no hay otras etiquetas emitiendo "coto")

3. **Multiplicaci√≥n de probabilidades**:
   ```math
   P(Estado_0, NOM) \times C(NOM, "coto") = 0.5 \times 1 = 0.5
   ```

Entonces hemos calculado la probabilidad de que el primer tag sea [NOM] y de que este [NOM] emita nuestra palabra, "coto". La probabilidad ha sido asignada a [NOM], ya que no hay m√°s opciones, definiendo nuestro **camino de Viterbi**

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741477795/viterbipath_ujjxod.jpg" alt="Definiendo el camino de viterbi">

Nuestro estado pasa a [NOM] y conocemos el siguiente estado visible, "privado". Para ver el siguiente movimiento el estado oculto vemos las probabiliadades de psar desde un estado [NOM] hasta cualquiera de los estados ocultos asociados a la palabra "privado". El algoritmo de Viterbi multiplicara la probabilidad de pasar desde [NOM] hasta cada una de las categor√≠as seleccionadas por la probabilidad de que "privado" sea emitido por alguna de estas categor√≠as.

```math
P(NOM, NOM) x C(NOM, "privado") = 0.1 x .3 = .03
P(NOM, VER) x C(VER, "privado") = 0.3 x .1 = .03
P(NOM, ADJ) x C(ADJ, "privado") = 0.4 x .7 = .28
P(NOM, ADV) x C(ADV, "privado") = 0.1 x .1 = .01
```

Dado el contexto de la frase el resultado sera [ADJ] entonces el algoritmo de VIterbi avanzar√° un estado a: "de", en el estado [ADJ], importante tambi√©n que multiplicara la probabilidad del tag anterior por la de cada uno de estos caminos, donde nuestro primer tag era [NOM] con .5

```math
P(estado_0, NOM, NOM) = 0.03 x .5 = .015
P(estado_0, NOM, VER) = 0.03 x .5 = .015
P(estado_0, NOM, ADJ) = 0.28 x .5 = .14
P(estado_0, NOM, ADV) = 0.01 x .5 = .0015
```

ENtonces tenemos en cada tag la probabilidad de que un determinado camino llegue hasta ellos, pero sin abandonar los otros

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741481046/viterbipath1_mcvdyo.jpg">

Ahora toca la palabra "de" sin embargo puede ser la cuarta letra del abecedario tambi√©n, por lo que computar√° 8 nuevas ramas.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741482306/viterbi_zyhuf4.webp">

Y el diagrama ser√°:

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741482358/viterbi1_n3wz7e.webp">

Aqu√≠ el algoritmo Viterbi ve que tenemos dos estados posibles con 4 ramas cada una, entonces podara las menos probables:

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741482548/viterbi2_roiixa.png">

Ahora es el turno de "caza":

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741482608/viterbi3_nvvcjx.webp">

Nos quedamos con la rama m√°s probable [PREP] -> [NOM], as√≠ decidimos c√∫al es nusetro **camino de Viterbi** permitiendonos tomar decisiones precisas y √≥ptimas en tareas donde la informaci√≥n real est√° oculta, como en el lenguaje, el sonido y la gen√©tica.

- El camino de Viterbi nos permite tomar decisiones precisas y √≥ptimas en tareas donde la informaci√≥n real est√° oculta, como en el lenguaje, el sonido y la gen√©tica.
- Porque en reconocimiento de voz hay muchas interpretaciones posibles, y Viterbi ayuda a elegir la m√°s coherente.
- Porque nos permite asignar etiquetas correctas a palabras ambiguas, mejorando tareas como la traducci√≥n autom√°tica o la generaci√≥n de texto.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1741483011/viterbi4_ka6kvo.webp">

## N-GRAMS - Autocompletado de texto

Estos modelos ayudan a asignar distintas probabilidades a cada una de las posibles secuencias que puede generar cada uno de ellos (n). Aunque existen modelos m√°s avanzados como las RNN y los Transformers.

Dado una frase "s" el siguiente token sea la palabra "w", esto es: P(w|s)

Si tenemos "la mesa esta" = "s" podemos ver en nuestro corpus y sacar una probabilidad para que el resultado sea "rota" pero esto es imposible para encontrar todas las frases posibles y ademas sea estadisticamente significativo.

UN modelo de 2-gram empleara la √∫ltima palabra de la frase para predecir la siguiente palabra, P(rota | La mesa del sal√≥n est√°), un modelo 2-gram calculara P(rota|est√°) buscando en nuestro corpus que se genere siendo la palabra "esta" la primera.

**Regla de la cadena:** Para saber la probabilidad de que ma√±ana haga calor y est√© soleado P(calor, soleado), podemos calcular la probabilidad de que haciendo calor est√© soleado P(soleado | calor) y multiplicar la probabilidad de que haga calor P(calor), seria como: P(calor, soleado) = P(soleado|calor) x P(calor)

Para hacer la prediccion en frases enteras como "hoy compr√© 2 pantalones"

P(hoy,compr√©, pantalores) = P(hoy) x P(compr√©|hoy) x P(dos|hoy compr√©) x P(pantalones| hoy compr√© dos); Buscara cuantas veces la palabra "compre" aparece despues de la palabra "hoy", lo dividira entre todas las ocurrencias de la palabra hoy y esto sera P(compre|hoy), igual pasara con las probabilidades del siguiente trigrama y del siguiente tetragrama para obtener la probabilidad completa de la frase.

Sin embargo nuestras frases no apareceran en nuestro corpus, es aqu√≠ que los 2-gram tienen sentido y a√±adiendo un simbolo al principio de la frase y al final tal que: **P(hoy|<\\s>)** y **P(<\/s>|hoy)** con estos simbolos los modelos pueden estimar la probabilidad de frases completas teniendo en cuenta la probabilidad de que la frase **comience** con una palabra determinada y tambien **termine** con una palabra determinada.

### Matriz de recuento (count matrix)
