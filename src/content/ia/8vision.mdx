---
title: 'Visi칩n 8: Aplicaci칩n de Objetos y Deep Learning'
description: ''
pubDate: 'Nov 08 2022'
heroImage: '/placeholder-hero.jpg'
url: '/ia'
---

import { Image } from 'astro:assets';

## Redes Neuronales Convolucionales (CNN)

Las redes neuronales convolucionales (CNN) son un tipo de arquitectura de deep learning dise침ada para procesar datos con una estructura de cuadr칤cula, como las im치genes. Su principal tarea es extraer autom치ticamente las caracter칤sticas m치s relevantes de las im치genes de entrenamiento y clasificarlas dentro de la red.

### Capa de Convoluci칩n

La capa de convoluci칩n act칰a como un filtro que extrae caracter칤sticas clave de la imagen, similar a un kernel en el procesamiento de im치genes. La diferencia es que en una CNN, la red aprende estos filtros autom치ticamente mediante entrenamiento con backpropagation.

<Image width={605} height={248} src="https://cdn.educalms.com/JTJGaExrQ2xkN0lTWm1ZTG1HWGRGNzhBJTNEJTNE-1721386137.png" alt="Ejemplo de convoluci칩n" />

A diferencia de los filtros tradicionales, en una CNN no hay un resultado por cada p칤xel, sino por cada ventana de convoluci칩n. Esto permite buscar patrones y caracter칤sticas en lugar de simplemente filtrar la imagen.

Podemos ver la convoluci칩n como un extractor de caracter칤sticas y como una t칠cnica para reducir dimensiones, resumiendo la informaci칩n visual.

<img src="https://cdn.educalms.com/QWFyZU1NeUFvdnZSaW9kUHNuZEZIUSUzRCUzRA==-1721386138.png" alt="Kernel de convoluci칩n" />

### Convoluci칩n en Im치genes RGB

Para una imagen en color, se usan tres kernels (uno por cada canal de color: rojo, verde y azul). La capa convolucional debe tener la misma profundidad que la imagen, y al final se suman los resultados de cada canal, generando una matriz bidimensional por cada convoluci칩n aplicada.

Ejemplo: Una imagen RGB de 5x5x3 requiere un kernel 3x3x3.

<img src="https://cdn.educalms.com/cDZLdyUyQmczUVZMbiUyQlBzSFU3MEZCY2clM0QlM0Q=-1721386138.png" alt="Convoluci칩n en im치genes" />

El proceso es el siguiente:

1. Se toma una ventana 3x3 en cada canal de la imagen (R, G, B) y se multiplica por su kernel correspondiente, generando un 칰nico valor por canal.
2. Se desplaza la ventana sobre la imagen, aplicando la misma operaci칩n y generando m치s valores.
3. Al finalizar, se obtienen tres matrices 3x3 (una por cada canal), cuyos valores se suman para obtener la salida final.

<img src="https://cdn.educalms.com/WWNQWkhsZGRTVkUzTkFjNjJXTlYlMkJ3JTNEJTNE-1721386138.png" alt="Proceso de convoluci칩n" />

### Capas Convolucionales y Aprendizaje Autom치tico

Cada neurona en la capa convolucional aplica una convoluci칩n con un kernel diferente. Esto permite detectar distintas caracter칤sticas en la imagen, como bordes, esquinas, sombras y brillos.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740319110/CNN_ywdryt.png" alt="Descripci칩n de la imagen" />

En el ejemplo anterior, una imagen de entrada de 32x32 pasa por una capa convolucional con 6 filtros de 5x5, generando 6 mapas de caracter칤sticas (feature maps) de 28x28.

Estos mapas representan la informaci칩n extra칤da autom치ticamente de la imagen y pueden utilizarse en una etapa posterior para la clasificaci칩n de objetos.

### Aprendizaje de los Filtros con Backpropagation

Los valores de los kernels no est치n predefinidos, sino que la red los aprende durante el entrenamiento mediante el algoritmo de backpropagation. Gracias a este proceso, la CNN ajusta sus filtros para detectar caracter칤sticas con mayor precisi칩n, sin intervenci칩n manual.

En resumen, las redes neuronales convolucionales permiten convertir una imagen de entrada en m칰ltiples mapas de caracter칤sticas de forma autom치tica, facilitando tareas como el reconocimiento de im치genes y la clasificaci칩n de objetos.

Aqu칤 tienes el texto corregido y mejor estructurado para tu blog:

---

### Capa de Downsampling, Subsampling o Pooling

En esta etapa, reducimos el tama침o de las matrices generadas en la capa anterior. Aplicamos una operaci칩n matem치tica o funci칩n estad칤stica a una ventana de p칤xeles, obteniendo un solo valor representativo.

Los dos principales m칠todos de pooling son:

1. **Max Pooling:** Se selecciona el valor m치ximo dentro de cada ventana.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740320067/Screenshot_20250223_101325_vaf80c.png" alt="Max Pooling" />

2. **Average Pooling:** Se calcula el promedio de los valores en la ventana.

   \[
   (128 + 220 + 221 + 12 + 45 + 16 + 154 + 42 + 8) / 9 = 95
   \]

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740320298/Screenshot_20250223_101808_mtz87m.png" alt="Average Pooling" />

Ambos m칠todos convierten un _feature map_ de \(5 \times 5\) en otro de \(3 \times 3\), reduciendo la informaci칩n sin perder caracter칤sticas clave.

### Combinando Capas Convolucionales y Pooling

La estrategia de una CNN consiste en alternar capas convolucionales y capas de pooling. Al aplicar convoluciones sobre los mapas reducidos, extraemos caracter칤sticas m치s complejas.

En las primeras capas, la CNN detecta bordes, esquinas o colores. En capas m치s profundas, aprende a identificar composiciones m치s avanzadas, como ruedas, puertas o ventanas, logrando un conocimiento m치s abstracto que los m칠todos tradicionales.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740321070/Screenshot_20250223_103058_ao32z3.png" alt="Estrategia de Capas en CNN" />

Tras aplicar pooling con una ventana de \(2 \times 2\), reducimos la dimensi칩n de los _feature maps_ a \(14 \times 14\) (_S1_).

Luego, una capa convolucional con 16 filtros de \(5 \times 5\) genera 16 _feature maps_ de \(10 \times 10\) (_C2_). Finalmente, aplicamos otro pooling con ventana de \(2 \times 2\), reduciendo a \(5 \times 5\) (_S2_).

### 쮺칩mo pasamos de aqu칤 a clasificar un d칤gito del 0 al 9?

Aqu칤 es donde entran las capas totalmente conectadas (_fully connected layers_). La salida de los 칰ltimos _feature maps_ se aplana y se conecta a una red neuronal densa. Esta red interpreta los patrones extra칤dos por la CNN y los asocia con un d칤gito espec칤fico.

游눠 **En la siguiente secci칩n exploraremos c칩mo funciona esta fase final en una CNN.**

---

### Capas totalmente conectadas (FC)

En este punto, tenemos una serie de _feature maps_ de tama침o reducido, que representan una extracci칩n compleja y abstracta de caracter칤sticas de nuestra imagen con una gran reducci칩n dimensional. Nuestro objetivo es que la red devuelva una clase concreta entre un conjunto de clases previamente definidas y etiquetadas. Para ello, debemos convertir estos _feature maps_ en una salida clasificable.

Aqu칤 entran en juego las _fully connected layers_ (FC), que conforman una red neuronal b치sica conocida como _feed-forward neural network_. En estas capas no hay ventanas, _kernels_ ni convoluciones, sino neuronas que reciben entradas, las multiplican por sus pesos y se activan en funci칩n del resultado de su funci칩n de activaci칩n.

Para alimentar esta capa, convertimos nuestras matrices (_feature maps_ finales) en un vector unidimensional (_flattened_) para que pueda ser procesado por las neuronas artificiales.

A diferencia de una capa convolucional, donde el procesamiento se realiza a nivel de ventana, en una capa totalmente conectada cada valor del vector de entrada est치 vinculado con todas las neuronas de la siguiente capa. En teor칤a, podr칤amos emplear cualquier clasificador lineal (_SVM, KNN, 츼rboles de Decisi칩n_), pero al utilizar una capa de neuronas artificiales b치sicas, logramos entrenar el modelo de principio a fin mediante _backpropagation_, permitiendo que la extracci칩n de caracter칤sticas y la clasificaci칩n se integren en un mismo proceso de entrenamiento basado en _deep learning_.

### Ejemplo pr치ctico

Supongamos que la salida de la fase de extracci칩n de caracter칤sticas de una CNN son 2 _feature maps_ de tama침o reducido (2x2). El proceso de clasificaci칩n sigue estos pasos:

1. **Aplanamiento**: Convertimos los _feature maps_ en un 칰nico vector unidimensional.
2. **Conexi칩n total**: Cada valor del vector est치 conectado con todas las neuronas artificiales de la siguiente capa (_fully connected_).
3. **Reducci칩n progresiva**: La primera capa transforma el vector de 8 posiciones en otro de 4, donde cada posici칩n representa la salida de una neurona, que a su vez ser치 la entrada de la siguiente capa totalmente conectada.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740324018/Screenshot_20250223_112009_aairuw.png" alt="Ejemplo de Fully Connected" />

La 칰ltima capa es la encargada de realizar la clasificaci칩n. En este caso, tenemos dos neuronas, lo que significa que la CNN est치 entrenada para distinguir entre dos clases. Cada neurona est치 asociada a una clase y, tras procesar las entradas con sus correspondientes pesos y funci칩n de activaci칩n, generar치 un valor entre 0 y 1. En este ejemplo:

- La neurona de la clase _gato_ tiene una activaci칩n del 98%, lo que indica una alta probabilidad de que la imagen sea un gato.
- La neurona de la clase _perro_ tiene una activaci칩n del 2%, indicando una baja probabilidad de que la imagen sea un perro.

### Resumen de la arquitectura de una CNN

1. **Capas convolucionales**: Formadas por neuronas que deslizan un _kernel_ sobre la imagen de entrada. Extraen _features_, comenzando con caracter칤sticas b치sicas (_l칤neas, bordes, sombras_) y avanzando a conceptos m치s abstractos (_formas, objetos, posiciones relativas_).

2. **Capas de _pooling_**: Reducen dimensionalmente los mapas de caracter칤sticas extra칤dos por las capas convolucionales, permitiendo aplicar una capa totalmente conectada de manera eficiente. No est치n compuestas por neuronas, sino por funciones estad칤sticas, quedando fuera del proceso de entrenamiento.

3. **Capas _fully connected_**: Procesan los _feature maps_ finales para clasificar el objeto. Convierten matrices en vectores unidimensionales y asignan una probabilidad a cada clase en funci칩n de la activaci칩n de la 칰ltima capa.

### Aplicaci칩n en clasificaci칩n de d칤gitos manuscritos (MNIST)

En el caso de la clasificaci칩n de d칤gitos manuscritos con MNIST, la arquitectura de la red sigue la misma l칩gica. En la fase de "classification", contamos con dos capas de neuronas conectadas que trabajan con el vector aplanado obtenido de la salida _S2_. En la 칰ltima capa totalmente conectada (_n2_), hay 10 neuronas, cada una representando un d칤gito del 0 al 9. La neurona con la activaci칩n m치s alta determinar치 la clase predicha por la red.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740325040/Screenshot_20250223_113710_bj0gns.png" alt="Clasificaci칩n en MNIST" />

### Proceso de Entrenamiento: Backpropagation y Stochastic Gradient Descent

Las redes neuronales convolucionales (CNN) combinan capas convolucionales y fully-connected. La diferencia radica en su aplicaci칩n: las capas convolucionales deslizan una ventana sobre la imagen (convolution), mientras que las fully-connected conectan directamente un vector de entrada con las neuronas.

Cada neurona en la red tiene los siguientes par치metros:

1. **N칰mero de entradas (inputs):** Cantidad de valores procesados en una 칰nica ejecuci칩n.
2. **Pesos de cada entrada (weights):** Coeficientes que ponderan cada entrada.
3. **Funci칩n de suma (net input function):** Suma ponderada de las entradas multiplicadas por sus pesos.
4. **Funci칩n de activaci칩n:** Aplica una transformaci칩n no lineal (como la sigmoide) a la suma ponderada para determinar la salida.
5. **Salida (output):** Resultado final de la neurona.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740325779/Screenshot_20250223_114932_sqymeg.png" alt="Ejemplo de Red Neuronal" />

### De la Selecci칩n Manual de Pesos al Aprendizaje Autom치tico

Inicialmente, los pesos de las neuronas deb칤an ajustarse manualmente, lo que hac칤a que las CNN se asemejaran a t칠cnicas cl치sicas de visi칩n artificial, donde cada neurona deb칤a programarse para extraer caracter칤sticas espec칤ficas. Sin embargo, esto cambi칩 con la introducci칩n de **Backpropagation** por Geoffrey Hinton en 1986. Este algoritmo permiti칩 que las redes neuronales aprendieran autom치ticamente.

El proceso de entrenamiento de una CNN con backpropagation sigue estos pasos:

1. **Inicializaci칩n aleatoria** de los pesos de la red neuronal.
2. **Clasificaci칩n inicial:** La red intenta clasificar im치genes sin conocimiento previo, generando resultados aleatorios.
3. **Comparaci칩n con el ground-truth:** Se mide la diferencia entre la salida de la red y la clasificaci칩n correcta.
4. **Ajuste de los pesos:** La CNN ajusta sus par치metros para reducir el error.
5. **Repetici칩n del proceso** hasta minimizar el error.

Backpropagation propaga el error desde las capas fully-connected hacia atr치s hasta las capas convolucionales, ajustando los pesos de manera eficiente.

### Optimizaci칩n con Stochastic Gradient Descent (SGD)

Una vez determinado c칩mo trasladar el error, es necesario un mecanismo de optimizaci칩n. **Stochastic Gradient Descent (SGD)** es el algoritmo m치s utilizado para entrenar CNNs. Su objetivo es encontrar el punto en el que los pesos minimicen el error y maximicen la precisi칩n.

Uno de los desaf칤os de SGD es evitar quedarse atrapado en **m칤nimos locales**, que pueden impedir alcanzar un 칩ptimo global. Gracias a backpropagation, es posible calcular el gradiente de cada peso y ajustarlo din치micamente para encontrar el m칤nimo absoluto del error.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740326818/Screenshot_20250223_120652_os1vaq.png" alt="Optimizaci칩n con SGD" />

El error de p칠rdida (_loss_) var칤a en funci칩n de los pesos, y el SGD busca el m칤nimo global de esta curva para maximizar la precisi칩n del modelo.

---

### Proceso de Entrenamiento: Backpropagation y Stochastic Gradient Descent

Las redes neuronales convolucionales (CNN) combinan capas convolucionales y fully-connected. La diferencia radica en su aplicaci칩n: las capas convolucionales deslizan una ventana sobre la imagen (convolution), mientras que las fully-connected conectan directamente un vector de entrada con las neuronas.

Cada neurona en la red tiene los siguientes par치metros:

1. **N칰mero de entradas (inputs):** Cantidad de valores procesados en una 칰nica ejecuci칩n.
2. **Pesos de cada entrada (weights):** Coeficientes que ponderan cada entrada.
3. **Funci칩n de suma (net input function):** Suma ponderada de las entradas multiplicadas por sus pesos.
4. **Funci칩n de activaci칩n:** Aplica una transformaci칩n no lineal (como la sigmoide) a la suma ponderada para determinar la salida.
5. **Salida (output):** Resultado final de la neurona.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740325779/Screenshot_20250223_114932_sqymeg.png" alt="Ejemplo de Red Neuronal" />

### De la Selecci칩n Manual de Pesos al Aprendizaje Autom치tico

Inicialmente, los pesos de las neuronas deb칤an ajustarse manualmente, lo que hac칤a que las CNN se asemejaran a t칠cnicas cl치sicas de visi칩n artificial, donde cada neurona deb칤a programarse para extraer caracter칤sticas espec칤ficas. Sin embargo, esto cambi칩 con la introducci칩n de **Backpropagation** por Geoffrey Hinton en 1986. Este algoritmo permiti칩 que las redes neuronales aprendieran autom치ticamente.

El proceso de entrenamiento de una CNN con backpropagation sigue estos pasos:

1. **Inicializaci칩n aleatoria** de los pesos de la red neuronal.
2. **Clasificaci칩n inicial:** La red intenta clasificar im치genes sin conocimiento previo, generando resultados aleatorios.
3. **Comparaci칩n con el ground-truth:** Se mide la diferencia entre la salida de la red y la clasificaci칩n correcta.
4. **Ajuste de los pesos:** La CNN ajusta sus par치metros para reducir el error.
5. **Repetici칩n del proceso** hasta minimizar el error.

Backpropagation propaga el error desde las capas fully-connected hacia atr치s hasta las capas convolucionales, ajustando los pesos de manera eficiente.

### Optimizaci칩n con Stochastic Gradient Descent (SGD)

Una vez determinado c칩mo trasladar el error, es necesario un mecanismo de optimizaci칩n. **Stochastic Gradient Descent (SGD)** es el algoritmo m치s utilizado para entrenar CNNs. Su objetivo es encontrar el punto en el que los pesos minimicen el error y maximicen la precisi칩n.

Uno de los desaf칤os de SGD es evitar quedarse atrapado en **m칤nimos locales**, que pueden impedir alcanzar un 칩ptimo global. Gracias a backpropagation, es posible calcular el gradiente de cada peso y ajustarlo din치micamente para encontrar el m칤nimo absoluto del error.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740326818/Screenshot_20250223_120652_os1vaq.png" alt="Optimizaci칩n con SGD" />

El error de p칠rdida (_loss_) var칤a en funci칩n de los pesos, y el SGD busca el m칤nimo global de esta curva para maximizar la precisi칩n del modelo.

---

### Modelos de CNN para Clasificaci칩n de Objetos

Las arquitecturas de redes neuronales convolucionales (CNN) pueden variar enormemente en funci칩n de varias decisiones clave:

1. **N칰mero de capas**: convolucionales, de pooling y fully-connected.
2. **Cantidad de neuronas** en cada capa y su distribuci칩n.
3. **Tama침o de los kernels** en las capas convolucionales y estrategia de desplazamiento (stride).
4. **Estrategia de downsampling**: tama침o, m칠todo y pol칤tica de aplicaci칩n.
5. **Algoritmo de entrenamiento** y configuraci칩n de hiperpar치metros.

Algunas arquitecturas est치n optimizadas para dispositivos m칩viles (eficiencia), mientras que otras priorizan la precisi칩n y requieren m칰ltiples GPUs.

<img src="https://res.cloudinary.com/djc1umong/image/upload/v1740409619/Screenshot_20250224_110650_oxmp0s.png" alt="Ejemplo de arquitectura CNN" />
