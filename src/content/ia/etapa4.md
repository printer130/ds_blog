---
title: 'ETAPA 4: Segundo invierno de la IA (1988 - 1993)'
description: 'En lugar de proporcionar a la máquina toda la información necesaria para resolver un problema, como exigían los sistemas expertos, su enfoque se basaba en **calcular las probabilidades de distintas traducciones para una frase y elegir la mejor opción**.'
pubDate: 'Nov 08 2022'
heroImage: '/placeholder-hero.jpg'
url: '/ia'
---

En 1988, el **Watson Research Center** de IBM inició un proyecto sobre **traducción automática**, proponiendo abandonar la lógica formal de los sistemas expertos. En lugar de proporcionar a la máquina toda la información necesaria para resolver un problema, como exigían los sistemas expertos, su enfoque se basaba en **calcular las probabilidades de distintas traducciones para una frase y elegir la mejor opción**.

### Avances en Deep Learning y visión artificial

En la misma época, **Yann LeCun** realizó una de las mayores contribuciones al **Deep Learning** y la **visión artificial**. Décadas después, en 2018, recibiría el **Premio Turing** junto a Geoffrey Hinton por sus aportes a la inteligencia artificial. Su innovación clave fueron las **Redes Neuronales Convolucionales (CNNs)**, inspiradas en modelos biológicos del cerebro humano.

LeCun combinó dos conceptos fundamentales:

1. **Organización similar a la visión humana:** diseñó una red neuronal con capas iniciales especializadas en la detección de bordes, brillos, colores y sombras.
2. **Profundidad en el aprendizaje:** utilizó **capas más profundas** para convertir la información visual en **representaciones abstractas y perceptibles**, dando origen al término **Deep Learning**.

Para entrenar estas redes, aprovechó el **algoritmo de retropropagación (Backpropagation)** desarrollado por Hinton. Como prueba de concepto, creó un modelo para **leer automáticamente dígitos manuscritos en cheques bancarios**, con el fin de **automatizar el reconocimiento de códigos postales**. Para este entrenamiento, desarrollaron la **base de datos MNIST**, que sigue siendo un estándar en la evaluación de CNNs.

En la década de 1990, el **10% de los cheques en EE.UU.** eran clasificados automáticamente mediante una versión optimizada de esta arquitectura de redes convolucionales.

### IA en el ajedrez y el debate sobre el enfoque simbólico

En 1989, **IBM** desarrolló **Deep Thought**, una IA que logró vencer a maestros de ajedrez, marcando un hito en la historia de la inteligencia artificial aplicada al juego.

En 1990, **Rodney Brooks** publicó _"Elephants Don't Play Chess"_, donde criticaba el enfoque de la IA basado en **lenguajes formales y simbólicos**. Según Brooks, intentar modelar el conocimiento del mundo con reglas predefinidas era un error, ya que la **neurociencia había demostrado que el cerebro biológico no funciona de esa manera**.

Brooks argumentaba que un algoritmo inteligente debía contar con **propiocepción y capacidad de aprendizaje**, en lugar de limitarse a seguir reglas fijas para resolver problemas específicos, como lo hacían los sistemas expertos. Este pensamiento impulsó el desarrollo de enfoques más dinámicos y adaptativos en la IA.
