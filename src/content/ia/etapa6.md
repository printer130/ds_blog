---
title: 'ETAPA 6: La Era del Big Data y el Deep Learning: Desde 2012'
description: 'Estos avances marcaron el inicio de una era dominada por la IA y el Deep Learning, impactando áreas como la visión por computadora, el procesamiento de lenguaje natural, la automatización y la conducción autónoma.'
pubDate: 'Nov 08 2022'
heroImage: '/placeholder-hero.jpg'
url: '/ia'
---

Estos avances marcaron el inicio de una era dominada por la IA y el Deep Learning, impactando áreas como la visión por computadora, el procesamiento de lenguaje natural, la automatización y la conducción autónoma.

### **El Auge del Deep Learning**

Deep Learning es un subcampo del Machine Learning basado en redes neuronales artificiales, inspiradas en la estructura del cerebro humano. Estas redes están compuestas por múltiples capas de perceptrones interconectados, donde cada capa procesa la información recibida de la anterior y la transmite a la siguiente, permitiendo un aprendizaje profundo y complejo.

---

### **CUDA y el Poder de las GPUs**

Los ingenieros de NVIDIA comenzaron a explorar el uso de la computación en paralelo más allá del renderizado 3D. Publicaron el marco CUDA (Compute Unified Device Architecture), permitiendo a los desarrolladores ejecutar cálculos matriciales sobre GPUs, optimizando significativamente los procesos de aprendizaje automático.

---

### **2012: El Impacto de AlexNet**

En 2012, Geoffrey Hinton y Alex Krizhevsky presentaron AlexNet en la competencia ImageNet Challenge. Esta red neuronal utilizaba la retropropagación en una GPU, logrando un 84.7% de precisión en la clasificación de imágenes. Su implementación aceleró el entrenamiento en un factor de 6, reduciendo el tiempo de semanas a horas. The Economist destacó: _"De repente, la industria tecnológica en su conjunto comenzó a prestar atención al Deep Learning"._

En el mismo año, Andrew Ng exploró el aprendizaje no supervisado. Al mostrar 10 millones de imágenes a una red neuronal sin proporcionar resultados específicos, descubrió que la red podía identificar patrones por sí sola, como reconocer la imagen de un gato activando una neurona específica.

También en 2012, el Dynamic Design Lab presentó _Shelley_, un automóvil de carreras autónomo diseñado para reaccionar a eventos imprevistos, avanzando en la seguridad de vehículos autónomos y facilitando su regulación legal.

---

### **2013: NEIL y el Aprendizaje No Supervisado**

La Universidad de Carnegie Mellon presentó _NEIL_ (Never Ending Image Learner), un sistema de IA que generaba conocimiento sobre relaciones del mundo real sin intervención humana. NEIL aprendió, por ejemplo, que los escolares usan uniforme, los perros tienen dueños y los ukeleles están asociados con la playa.

---

### **2014: Avances en Hardware y Redes Neuronales**

- **NVIDIA lanzó cuDNN**, una biblioteca optimizada para la retropropagación y convoluciones, utilizada en frameworks como TensorFlow, Caffe y PyTorch.
- **VGGNet**, desarrollada por la Universidad de Oxford, ganó la competencia ImageNet al mejorar la localización de objetos, reduciendo el error a menos del 10%.
- **El coche autónomo de Google** aprobó un examen de conducción en Nevada tras recorrer 1 millón de kilómetros, marcando un hito en la regulación de la conducción autónoma.
- **Microsoft lanzó Cortana**, un asistente virtual capaz de gestionar archivos, ejecutar comandos y realizar búsquedas.
- **Amazon presentó Alexa**, un asistente domótico cuyo nombre fue elegido estratégicamente por su fonema "X", facilitando su reconocimiento por el sistema.
- **Eugene Goostman** superó el Test de Turing al simular ser un niño de 13 años, aunque la comunidad lo consideró una prueba sesgada.
- **Tesla lanzó AutoPilot**, aunque inicialmente presentaba limitaciones y requería intervención humana.
- **Ian Goodfellow, bajo la dirección de Yoshua Bengio, propuso las Redes Generativas Adversarias (GANs)**, donde una red generativa crea imágenes y una red adversaria las evalúa, revolucionando la síntesis de imágenes.

---

### **2015: Ética, Conducción Autónoma y Nuevas Arquitecturas**

- Durante el congreso IJCAI, Elon Musk, Stephen Hawking, Steve Wozniak y 3,000 expertos firmaron una carta pidiendo la prohibición de armas autónomas basadas en IA.
- **Un coche autónomo recorrió 5,500 km de Nueva York a San Francisco con solo un 1% de intervención humana.**
- **Tesla Motors, en su versión 7, alcanzó el nivel 3 de autonomía**, un hito en la conducción autónoma en condiciones reales.
- **Google desarrolló DistBelief**, precursor de TensorFlow, diseñado para ejecutar modelos en múltiples CPUs y GPUs simultáneamente, optimizando el algoritmo de retropropagación de Hinton.
- **YOLO (You Only Look Once) revolucionó la detección de objetos**, permitiendo procesar imágenes en tiempo real a 25 fps mediante un enfoque de cuadrícula.
- **Viola-Jones se implementó en móviles y navegadores web**, facilitando la detección de rostros en videollamadas en tiempo real.

---

**La evolución de la Inteligencia Artificial: Hitos clave en su desarrollo**

### 2016: AlphaGo y la revolución del Deep Learning

En 2016, DeepMind presentó **AlphaGo**, un sistema de inteligencia artificial diseñado para jugar **Go**, un juego de mesa chino considerado mucho más complejo que el ajedrez debido a sus innumerables combinaciones posibles. La clave del éxito de AlphaGo fue la combinación de **árboles de búsqueda** con **redes neuronales profundas**:

- **Policy Network:** Selecciona el siguiente movimiento utilizando el tablero como entrada.
- **Value Network:** Predice el ganador de la partida.

En la final, AlphaGo se enfrentó al campeón coreano **Lee Sedol**, ganando con un marcador de **4-1**. Para su ejecución, el sistema requirió **1,920 CPUs y 280 GPUs**, alojados en la nube de Google.

Ese mismo año, el **Deep Learning** se democratizó a nivel global. Google lanzó un servicio de IA en la nube, permitiendo a cualquier usuario entrenar modelos de **visión artificial** o **procesamiento de lenguaje natural (NLP)** bajo un esquema **SaaS**.

Además, Google presentó sus **Tensor Processing Units (TPUs)**, chips diseñados específicamente para la ejecución eficiente de redes neuronales, especialmente en tareas de visión artificial con **redes convolucionales**.

En paralelo, **Hanson Robotics** desarrolló **Sophia**, un robot humanoide que combinaba las últimas tecnologías en IA y que incluso obtuvo la ciudadanía saudí.

Microsoft también incursionó en IA conversacional con el chatbot **Tay**, diseñado para interactuar en Twitter. Sin embargo, en solo **16 horas** de entrenamiento, Tay comenzó a emitir comentarios racistas, sexistas y antisemitas debido al aprendizaje no supervisado basado en interacciones con los usuarios. Microsoft lo desconectó, pero tras un intento fallido de reactivación, el proyecto fue abandonado.

---

### 2017: AlphaGo Zero y el dominio absoluto de la IA

En 2017 nació **Libratus**, un bot diseñado para jugar **póker** con una innovadora estrategia basada en **teoría de juegos (GTO)**. Su capacidad para **ajustar dinámicamente su estrategia** le permitió vencer a los mejores jugadores humanos. Esta tecnología fue posteriormente utilizada por el ejército de EE.UU. para mejorar el despliegue de tropas mediante **estrategias de IA**.

Ese mismo año, DeepMind lanzó **AlphaGo Zero**, una versión mejorada que aprendió **únicamente jugando contra sí misma**. Los resultados fueron impactantes:

- Venció a su predecesor **100-0**.
- En **40 días**, superó todos los algoritmos de Go de la historia.
- En **1 día de entrenamiento**, ya podía vencer a cualquier jugador humano.

Lo más sorprendente fue que AlphaGo Zero descubrió estrategias **nunca antes vistas** por jugadores humanos. Esto generó un debate en la comunidad científica sobre el papel de la IA en la toma de decisiones estratégicas y su capacidad para encontrar soluciones innovadoras en problemas sin reglas predefinidas.

Por su parte, Facebook desarrolló **dos chatbots con una lógica similar a AlphaGo**, diseñados para negociar objetos virtuales entre sí. Sin embargo, los bots terminaron desarrollando un lenguaje propio que solo ellos entendían, lo que llevó a la cancelación del proyecto.

---

### Redes Generativas y la creación de rostros sintéticos

Ese mismo año, el proyecto **This Person Does Not Exist** empleó **Redes Generativas Adversarias (GANs)**, desarrolladas por **Ian Goodfellow**, para generar **rostros humanos ficticios** con un realismo sin precedentes. Esta tecnología abrió la puerta a nuevas aplicaciones en diseño, entretenimiento y ciberseguridad.

---

### 2018: Avances Clave en Inteligencia Artificial

**Alibaba y Microsoft en Comprensión Lectora**
Alibaba y Microsoft desarrollaron inteligencias artificiales para examinarse en el test de comprensión lectora SQuAD. Este examen consta de 100,000 preguntas basadas en 500 artículos de Wikipedia. Alibaba obtuvo una puntuación de 82.4 y Microsoft 82.6, demostrando la capacidad de estas IAs para responder preguntas con precisión. Estas tecnologías se implementaron en sistemas de atención al cliente con resultados positivos.

**China y la Inversión en IA**
China invirtió 150 mil millones de dólares en inteligencia artificial, con el respaldo de gigantes tecnológicos como Alibaba y Tencent.

**Facebook y la Segmentación de Objetos**
Facebook presentó Detectron, una herramienta basada en Caffe2, especializada en la segmentación de objetos. A diferencia de los métodos anteriores que solo identificaban los objetos mediante rectángulos, Detectron permitió identificar cada píxel de los objetos detectados. Posteriormente, Facebook lanzó Mask-RCNN para mejorar esta técnica y la puso a disposición del público en la nube.

**Google y TensorFlow**
Google continuó el desarrollo de TensorFlow, consolidándolo como uno de los marcos más avanzados para la implementación de modelos de IA.

**Autobús Autónomo de Nivel 5**
En marzo de 2018, Suiza lanzó "Trapizio", el primer autobús autónomo de nivel 5 impulsado por tecnología CNN Supercharged. Este vehículo operaba en una línea estándar con pasajeros, marcando un hito en la movilidad autónoma.

**CIMON: Asistente de Astronautas**
IBM diseñó CIMON, un asistente basado en IA que fue enviado al espacio en junio de 2018. Su objetivo era proporcionar asistencia en tiempo real a los astronautas, respondiendo preguntas y ofreciendo instrucciones como un "cerebro flotante".

**GPT: Revolución en el Procesamiento del Lenguaje Natural**
En junio de 2018, OpenAI, con el respaldo de Elon Musk y Sam Altman, presentó el modelo Generative Pre-Trained Transformer (GPT). Su entrenamiento se realizó en dos fases:

1. **Fase 1:** Se permitió que la IA estudiara 700 libros para modelar por sí misma el lenguaje humano.
2. **Fase 2:** Se le proporcionó texto etiquetado para realizar tareas como clasificación y resumen de textos.

GPT superó al mejor modelo de su época en 9 de 12 pruebas de procesamiento de lenguaje natural (NLP), estableciendo un nuevo estándar en la formación de modelos generalistas de lenguaje.

**BERT: Avance de Google en NLP**
Google lanzó BERT, un modelo diseñado para mejorar la comprensión del contexto en textos. A diferencia de GPT, que leía textos sin contexto, BERT utilizaba bidireccionalidad para analizar las palabras en relación con su entorno.

BERT logró una puntuación de 87.43 en el test de Stanford SQuAD y podía entrenarse en solo 30 minutos con las TPUs de Google o en unas pocas horas con GPUs convencionales.

**Facebook y la Visión Artificial**
En diciembre de 2018, Facebook incorporó redes neuronales convolucionales (CNN) para detectar contenido explícito en imágenes. Inicialmente, el sistema arrojaba muchos falsos positivos, pero con reentrenamientos mejoró significativamente su precisión.

**Waymo y los Taxis Autónomos**
Google, a través de Waymo, lanzó su servicio de taxis autónomos tras completar 10 millones de kilómetros de pruebas. En Phoenix, Waymo obtuvo su permiso de conducción para operar, aunque solo el 10% de los viajes en 2020 se realizaron completamente sin intervención humana.

---

### 2019: Avances en Inteligencia Artificial

**Febrero 2019 – DeepMind y la clonación de voz**  
DeepMind presentó un sistema de síntesis de voz basado en IA capaz de clonar cualquier voz humana en solo cinco segundos de escucha. Una vez entrenado con múltiples hablantes, el modelo podía replicar cualquier tono y estilo con naturalidad y realismo.

**2019 – OpenAI y el aprendizaje profundo no supervisado**  
OpenAI avanzó en la reducción de la necesidad de datos etiquetados para el entrenamiento de IA. Su proyecto _Dactyl_ controlaba una mano robótica a través de imágenes como entrada y movimientos como salida. Inicialmente, el aprendizaje no supervisado tardaba demasiado, por lo que incorporaron 50 horas de supervisión. El resultado: la IA descubrió seis formas de agarrar y manipular objetos sin haber sido programada explícitamente para ello. Posteriormente, se desafió a _Dactyl_ a resolver un cubo de Rubik, alcanzando un éxito del 60%.

**Febrero 2019 – OpenAI presenta GPT-2**  
OpenAI lanzó GPT-2, un modelo de lenguaje capaz de generar texto automáticamente a partir de una frase inicial. Destacaba por su capacidad de improvisar, responder preguntas, traducir idiomas y analizar discursos de manera conversacional. Entrenado con 45 millones de páginas web, podía resumir información sin necesidad de ajuste posterior. Sin embargo, solo el 50% de los textos generados eran coherentes; el resto carecía de sentido. Debido a posibles riesgos, OpenAI decidió liberar solo una parte del algoritmo. NVIDIA expresó escepticismo sobre el impacto real de GPT-2.

**Marzo 2019 – Premio Alan Turing y la consolidación del Deep Learning**  
La Asociación de Máquinas de Cómputo (ACM) otorgó el Premio Alan Turing a Yoshua Bengio, Geoffrey Hinton y Yann LeCun por sus contribuciones fundamentales al Deep Learning, consolidando su papel en la computación moderna.  
Avances clave del Deep Learning:

- Redes neuronales convolucionales (_LeCun, década de 1980_).
- Algoritmo de _Backpropagation_ (_Hinton, década de 1980_).
- Uso de GPUs para el entrenamiento de redes neuronales (_Hinton y Krizhevsky, 2012_).
- Primeras GAN (_Bengio y Goodfellow, 2014_).

**Mayo 2019 – IA médica y diagnóstico de cáncer**  
Google, en colaboración con médicos, entrenó una IA basada en redes neuronales convolucionales para detectar indicios de cáncer de pulmón en tomografías. Comparada con seis radiólogos, la IA logró un 94% de precisión en la detección temprana, reduciendo significativamente el tiempo de diagnóstico.

**Mayo 2019 – Samsung y la animación de rostros**  
Samsung desarrolló un sistema capaz de generar animaciones a partir de una sola fotografía. La IA movía el rostro y sincronizaba los labios con el audio, logrando una apariencia realista sin necesidad de un escaneo 3D previo.

---

### **Resumen (2012 - 2024)**

...

---
